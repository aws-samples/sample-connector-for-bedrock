{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Bedrock Connector Doc","text":""},{"location":"how-it-works/","title":"How it works","text":""},{"location":"deploy/","title":"Quick Deploy BRConnector using Cloudformation","text":""},{"location":"deploy/#supported-region","title":"Supported Region","text":"<p>Cloudformation template are verified in following regions:</p> <ul> <li>us-east-1</li> <li>us-west-2</li> </ul>"},{"location":"deploy/#prerequisites","title":"Prerequisites","text":"<p>Enable Claude 3 Sonnet or Haiku in your region - If you are new to using Anthropic models, go to the\u00a0Amazon Bedrock console\u00a0and choose\u00a0Model access\u00a0on the bottom left pane. Request access separately for Claude 3 Sonnet or Haiku.</p>"},{"location":"deploy/#components","title":"Components","text":"<p>Following key components will be included in this Cloudformation template: </p> <ul> <li>Cloudfront</li> <li>BRConnector on Lambda or EC2</li> <li>RDS PostgreSQL or PostgreSQL container on EC2</li> <li>ECR with pull through cache enabled</li> </ul>"},{"location":"deploy/#deploy-guide","title":"Deploy Guide","text":"<ul> <li>Download quick-build-brconnector.yaml and upload to Cloudformation console or click this button to launch directly.</li> </ul> <ul> <li>VPC parameters<ul> <li>Choose to create a new VPC or a existing VPC </li> <li>Choose one PUBLIC subnet for EC2 and two PRIVATE subnets for Lambda and RDS (subnet group need 2 AZ at least)</li> </ul> </li> </ul> <ul> <li>Compute parameters<ul> <li>Choose ComputeType for BRConnector, Lambda or EC2</li> <li>For EC2 settings<ul> <li>Now only support Amazon Linux 2023</li> <li>You could choose to create PostgreSQL as container in same EC2 (<code>StandaloneDB</code> to false), or create standalone RDS PostgreSQL as backend (<code>StandaloneDB</code> to true)</li> </ul> </li> <li>For Lambda settings<ul> <li>PUBLIC Function URL will be used. Please ensure this security setting is acceptable</li> <li>Define your private repository name prefix string</li> <li>Always create RDS PostgreSQL (<code>StandaloneDB</code> to true)</li> </ul> </li> </ul> </li> </ul> <ul> <li>PostgreSQL parameters<ul> <li>Default PostgreSQL password is <code>mysecretpassword</code></li> <li>If you choose <code>StandaloneDB</code> to false, PostgreSQL will running on EC2 as container. RDS PostgreSQL will be create if this option is true.</li> <li>Keep others as default</li> </ul> </li> </ul> <ul> <li>Debugging parameters<ul> <li>If you choose Lambda as ComputeType, you could choose to delete EC2 after all resources deploy successfully. This EC2 is used for compiling and building BRConnector container temporarily. </li> <li>Don\u2019t delete EC2 if you choose EC2 as ComputeType</li> <li>If you set <code>true</code> to AutoUpdateBRConnector, one script will be add to ec2 crontab</li> </ul> </li> </ul> <ul> <li>Until deploy successfully, go to output page and copy Cloudfront URL and first user key to your bedrock client settings page.</li> </ul> <ul> <li>Also you could connect to <code>BRConnector</code> EC2 instance with SSM Session Manager (docs)</li> </ul>"},{"location":"deploy/#update-brconnector","title":"Update BRConnector","text":""},{"location":"deploy/#autoupdate-is-true","title":"AutoUpdate is true","text":"<ul> <li>Check your ECR settings, if has rules in pull through cache page, you have enabled this feature to update ECR image with upstream repo automatically.</li> <li>Go to codebuild page, one project will be triggered to build regularly to update your lambda image automatically</li> <li>Images in EC2 will be updated using state manager in SSM automatically.</li> </ul>"},{"location":"deploy/#autoupdate-is-false","title":"AutoUpdate is false","text":"<ul> <li>Check your ECR settings, if has rules in pull through cache page, you have enabled this feature to update ECR image with upstream repo automatically.</li> <li>Go to codebuild page, one project could be triggered to update your lambda image manually. Click <code>Start build</code> to update lambda image.</li> <li>Images in EC2 will NOT be updated using state manager in SSM automatically due to no association created. Reference document in SSM to execute commands in EC2 manually.</li> </ul>"},{"location":"deploy/#ecr-without-pull-through-cache-enabled-only-for-previous-cfn-version","title":"ECR without pull through cache enabled (only for previous cfn version)","text":"<ul> <li> <p>following this script to update image manually if you do not enable ECR pull through cache <pre><code>export AWS_DEFAULT_REGION=us-west-2\nexport ACCOUNT_ID=123456789012\nexport PrivateECRRepository=your_private_repo_name\n\naws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\n\n# pull/tag/push arm64 image for lambda\ndocker pull --platform=linux/arm64 public.ecr.aws/x6u9o2u4/sample-connector-for-bedrock-lambda\ndocker tag public.ecr.aws/x6u9o2u4/sample-connector-for-bedrock-lambda ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:arm64\ndocker push ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:arm64\n\n# pull/tag/push amd64 image for lambda\ndocker pull --platform=linux/amd64 public.ecr.aws/x6u9o2u4/sample-connector-for-bedrock-lambda\ndocker tag public.ecr.aws/x6u9o2u4/sample-connector-for-bedrock-lambda ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:amd64\ndocker push ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:amd64\n\n# create/push manifest file\ndocker manifest create ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:latest --amend ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:arm64 --amend ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:amd64\ndocker manifest annotate ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:latest ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:arm64 --os linux --arch arm64\ndocker manifest annotate ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:latest ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:amd64 --os linux --arch amd64\ndocker manifest push ${ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${PrivateECRRepository}:latest\n</code></pre></p> </li> <li> <p>update lambda image with correct architecture</p> </li> <li>or login to ec2 to update local image and restart brconnector container</li> </ul>"},{"location":"deploy/#migrating-to-new-rds-postgresql-database","title":"Migrating to new RDS PostgreSQL database","text":"<p>working \u2026</p>"},{"location":"guide/aws-executor/","title":"AWS command executor","text":"<p>With this Provider, you can execute AWS command using natural language and get the execution results.</p> <p>The role for executing AWS commands and the role for running BRConnector are currently the same, so you need to grant the appropriate permissions to the current role.</p> <p>[!IMPORTANT] Do not grant write permissions to critical resources, as the command lines parsed by the current AI are not stable. However, you can still refer to the command lines suggested by the AI.</p>"},{"location":"guide/aws-executor/#model-configuration","title":"Model configuration","text":"<p>The parameter configuration is as follows:</p> <p>Name: some-model</p> <p>Provider: aws-executor</p> <p>Configuration:</p> <pre><code>{\n  \"localLlmModel\": \"claude-3-sonnet\"\n}\n</code></pre> <p>localLlmModel must be configured as a model that supports function calling and already exists in BRConnector.</p> <p>[!Note] You need to configure a claude3+ model provided by the bedrock-converse provider, as other models do not yet have the capability for function calling. The default claude3 model in the system is not driven by converse. If you use these models, you need to update the original configuration to the bedrock-converse provider. Please note to modify the key \u2018model_id\u2019 to \u2018modelId\u2019.</p> <p>And You must install aws cli v2 in the BRConnector host.</p>"},{"location":"guide/aws-executor/#install-aws-cli-in-docker-image","title":"Install AWS CLI  in Docker image","text":"<p>In this demo, the AWS CLI is not included in the Docker image, and you can install it yourself with the following command.</p> <pre><code>docker exec -it &lt;your-docker-instance&gt;  sh\n</code></pre> <pre><code>apt update &amp;&amp; apt install -y awscli\n</code></pre>"},{"location":"guide/aws-executor/#screenshots-in-brclient","title":"Screenshots in BRClient","text":""},{"location":"guide/bedrock-knowledge-base/","title":"How to Support Knowledge Bases for Amazon Bedrock","text":""},{"location":"guide/bedrock-knowledge-base/#create-a-knowledge-base-instance","title":"Create a knowledge base instance","text":"<p>Refer to this document: Create a knowledge base</p>"},{"location":"guide/bedrock-knowledge-base/#create-a-custom-model","title":"Create a custom model","text":"<p>Moel Configuration:</p> <pre><code>{\n  \"region\": \"&lt;your-region&gt;\",\n  \"summaryModel\": \"claude-3-sonnet\",\n  \"knowledgeBaseId\": \"&lt;your-kb-id&gt;\"\n}\n</code></pre> <ul> <li>knowledgeBaseId: the knowledge base id.</li> <li>summaryModel: support claude-3-sonnet, claude-3-haiku or claude-3-opus</li> </ul>"},{"location":"guide/bedrock-knowledge-base/#api-calls","title":"API Calls","text":"<p>You can use normal api invoke, the Bedrock connector will pop last message to chat with the knowledge base.</p> <pre><code>POST /v1/chat/completions\nContent-Type: application/json\nAuthorization: Bearer br_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n{\n  \"model\": \"your-custom-model-name\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"how to protect s3 data?\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guide/bedrock-knowledge-base/#use-brclient","title":"Use BRClient","text":""},{"location":"guide/painter/","title":"Provider painter","text":""},{"location":"guide/painter/#configuration","title":"Configuration","text":"<p>Add a model in the /manager UI.</p> <p></p> <ul> <li>Name: Any words</li> <li>Provider: painter</li> <li>Multiple: disable</li> <li>Price-in: any number</li> <li>Price-out: any number</li> <li>Config: Please see the sample below.</li> </ul> <pre><code>{\n  \"regions\": [\n    \"us-east-1\", \"us-west-2\"\n  ],\n  \"s3Bucket\": \"&lt;your-bucket&gt;\",\n  \"s3Prefix\": \"&lt;your-prefix&gt;\",\n  \"s3Region\": \"us-east-1\",\n  \"sdModelId\": \"stability.stable-diffusion-xl-v1\",\n  \"llmModelId\": \"anthropic.claude-3-sonnet-20240229-v1:0\"\n}\n</code></pre> <p>Then grant this model to  group or apikey.</p>"},{"location":"guide/painter/#features-and-screenshots-in-brclient","title":"Features and Screenshots in BRClient","text":"<p>Features:</p> <ul> <li>Supports natural language conversation</li> <li>Supports multi-turn conversation to refine prompts</li> <li>Supports multiple languages</li> <li>Supports image size and aspect ratio ratio</li> </ul> <p>Screenshots in BRClient:</p> <p>BRClient has been built into the Docker image (since version 0.0.8). Access address: your-host:8866/brclient/</p> <p></p> <p></p>"},{"location":"guide/sagemaker-openai/","title":"How to properly deploy a Sagemaker model","text":""},{"location":"guide/sagemaker-openai/#input","title":"Input","text":"<p>To make the Sagemaker-deployed model compatible with the provider of sagemaker-openai, you need to ensure your model accepts input in the OpenAI API format, which is as follows:</p> <pre><code>{\n  \"model\": \"YourDefinedModel\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"hello\"\n    }\n  ],\n  \"stream\": true,\n  \"temperature\": 0.5,\n  \"presence_penalty\": 0,\n  \"frequency_penalty\": 0,\n  \"top_p\": 1,\n  \"max_tokens\": 4000\n}\n</code></pre>"},{"location":"guide/sagemaker-openai/#output","title":"Output","text":"<p>At the same time, you need to ensure your model\u2019s output also conforms to the OpenAI format. There are two output formats: streaming and non-streaming.</p>"},{"location":"guide/sagemaker-openai/#non-streaming","title":"Non-streaming","text":"<p>The non-streaming output format is as follows:</p> <pre><code>{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-3.5-turbo-0125\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21\n  }\n}\n</code></pre> <p>choices[0].message.content is required.</p>"},{"location":"guide/sagemaker-openai/#streaming","title":"Streaming","text":"<p>This is an example of streaming output, where each line represents a complete segment. Please do not truncate it. You must response this in your sagemaker custom inference function.</p> <pre><code>{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\n\n{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"logprobs\":null,\"finish_reason\":null}]}\n\n....\n\n{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}]}        \n</code></pre>"},{"location":"guide/sagemaker-openai/#sagemaker-custom-inference-deployment-resources","title":"Sagemaker Custom Inference Deployment Resources","text":"<p>You can use Sagemaker\u2019s custom inference code to perform format conversion. You can refer to these examples:</p> <p>https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb</p> <p>https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab3-optimize-llm/djl_accelerate_deploy_g5_12x_GPT_NeoX.ipynb</p> <p>https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/testing-custom-script.html</p>"},{"location":"guide/web-miner/","title":"Web miner with search engine","text":"<p>This Provider can turn your question into search keywords, obtain results through search engines, and then summarize them into corresponding answers.</p> <p>[!TIP] Please note, do not ask too many rounds of questions, because this Provider will summarize your previous prompts to keywords for searching.</p> <p>And since the results of the questions are too many, the BRClient will summarize the history, thereby losing the earliest user input.</p>"},{"location":"guide/web-miner/#model-configuration","title":"Model configuration","text":"<p>The parameter configuration is as follows:</p> <p>Name: some-model</p> <p>Provider: web-miner</p> <p>Configuration:</p> <pre><code>{\n  \"sites\": [\n    \"aws.amazon.com\",\n    \"www.amazonaws.cn\",\n    \"repost.aws\",\n    \"stackoverflow.com\"\n  ],\n  \"googleCSECX\": \"00xxxc000a2xxxxx\",\n  \"googleAPIKey\": \"AIxxxxxx_xxxxxxxx\",\n  \"localLlmModel\": \"claude-3-sonnet\"\n}\n</code></pre> <ul> <li>sites: Limit the search to these websites.</li> <li>googleAPIKey: Google API key.</li> <li>googleCSECX: Google custom search engine key.</li> <li>localLlmModel: must be configured as a model that supports function calling and already exists in BRConnector.</li> </ul> <p>Google CSE key apply</p>"},{"location":"guide/web-miner/#screenshots-in-brclient","title":"Screenshots in BRClient","text":""}]}