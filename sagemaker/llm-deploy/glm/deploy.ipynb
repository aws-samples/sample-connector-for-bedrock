{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy GLM 4 chat model to Sagemaker \n",
    "\n",
    "(Under development, dont try.)\n",
    "\n",
    "Deploy a model use OpenAI API schema to interact with.\n",
    "\n",
    "Model url: https://huggingface.co/THUDM/glm-4-9b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and upgrade dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init sagemaker parameters\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_code_prefix = \"chatglm4/accelerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The inference codes is placed in the ./code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!tar czvf model.tar.gz -C code ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve SageMaker LMI container image URI\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.28.0\"\n",
    ")\n",
    "\n",
    "\n",
    "print(image_uri)\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=s3_code_artifact, role=role)\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"  # \"ml.g5.2xlarge\" - #single GPU. really need one GPU for this since tensor split is '1'\n",
    "\n",
    "endpoint_name = \"glm-4-chat\"\n",
    "\n",
    "print(\"模型部署过程大约需要 7~8 分钟，请等待\" + \".\"*20)\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")\n",
    "\n",
    "print(\"模型部署已完成，可以继续执行后续步骤\" + \".\"*20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
