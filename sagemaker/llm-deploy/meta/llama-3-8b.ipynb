{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama3 to Samgemaker\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name wellxie to get Role path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('HuggingfaceExecuteRole',\n",
       " 'shenzhi-wang/Llama3-8B-Chinese-Chat',\n",
       " '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "import boto3\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "\n",
    "role_name = os.environ.get(\"role_name\")\n",
    "model_id = os.environ.get(\"model_id\")\n",
    "image_uri = sagemaker.huggingface.get_huggingface_llm_image_uri(\"huggingface\")\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role() # If you online sagemaker notebook\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]\n",
    "\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = \"llama-3-8b-tgi\"\n",
    "\n",
    "role_name, model_id, image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deloyment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "\n",
    "model = sagemaker.huggingface.HuggingFaceModel(\n",
    "  image_uri=image_uri,\n",
    "  role=role,\n",
    "  env={\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'SM_NUM_GPUS': \"1\", # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': \"4000\",  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': \"8192\",  # Max length of the generation (including input text)\n",
    "    # 'MAX_BATCH_TOTAL_TOKENS': \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    # 'MESSAGES_API_ENABLED': \"True\", # You should disable the messages API, or it will split a full json to chunk when you wanna streaming, but that's not streaming.\n",
    "    # 'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\", # Token used to login to huggingface Hub.\n",
    "  }\n",
    ")\n",
    "\n",
    "\n",
    "llm = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  endpoint_name=endpoint_name,\n",
    "  container_startup_health_check_timeout=1800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As spring awakens from its winter sleep,\n",
      "The world regains its vibrant hue and life,\n",
      "Buds burst forth, and flowers bloom with glee,\n",
      "In every color, a beauty to strife.\n",
      "\n",
      "The sun shines bright, and warmth fills the air,\n",
      "Birds sing their sweet melodies so free,\n",
      "A time of renewal, hope and joy are there,\n",
      "A season of growth, a time to be.\n",
      "\n",
      "The earth awakens from its winter's sleep,\n",
      "And all around, new life begins to creep,\n",
      "A time of promise, a time to reap,\n",
      "A season of beauty, a time to keep.\n",
      "\n",
      "So let us cherish this time of year,\n",
      "And let our spirits soar, without a fear.<|eot_id|>"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "\n",
    "\n",
    "endpoint_name = \"llama-3-8b-tgi\"\n",
    "\n",
    "\n",
    "# prompt = \"Write a sonnet about spring.\"\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant. Answer briefly!<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "write a sonnet about spring.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response_model = smr.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        { \n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {  \n",
    "                    \"top_p\": 0.6,\n",
    "                    \"max_new_tokens\": 8000, # Input validation error: `inputs` tokens + `max_new_tokens` must be <= 2048. So you must control you inputs length\n",
    "                    \"temperature\": 0.9, \n",
    "                    \"return_full_text\": False,\n",
    "                    \"stop\": [\"<|eot_id|>\"]\n",
    "                    },\n",
    "            \"stream\": True,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "\n",
    "stream = response_model.get(\"Body\")\n",
    "\n",
    "if stream:\n",
    "    chunk_str_full = ''\n",
    "    for event in stream:\n",
    "        chunk = event.get(\"PayloadPart\")\n",
    "        if chunk:\n",
    "            str00 =chunk.get(\"Bytes\").decode()\n",
    "            chunk_str_full = chunk_str_full + str00\n",
    "\n",
    "            if chunk_str_full.strip().endswith(\"}\") and chunk_str_full.strip().startswith(\"data:\"):\n",
    "                chunk_str_full = chunk_str_full.replace(\"data:\", \"\")\n",
    "                chunk_obj = json.loads(chunk_str_full) \n",
    "                chunk_str_full = \"\"\n",
    "                print(chunk_obj.get(\"token\").get(\"text\"), end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config BRConnector\n",
    "\n",
    "Once you have successfully configured Sagemaker, you can proceed with configuring the BRConnector.\n",
    "\n",
    "First, open the Manager UI and add a new model, configuring the parameters as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"regions\": [\n",
    "    \"us-west-2\"\n",
    "  ],\n",
    "  \"endpointName\": \"llama-3-8b-tgi\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "![image.png](./llama3-config.jpg)\n",
    "\n",
    "Remember to correct the region and endpoint name for your model deployment and authorize the model for a Group or API Key.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb-bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
